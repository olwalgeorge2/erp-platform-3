# Kafka Integration Summary

> **‚ö†Ô∏è DEPRECATED DOCUMENT**: This document describes the original Apache Kafka integration. The platform has been migrated to **Redpanda** for better performance and operational simplicity. See [`REDPANDA_MIGRATION.md`](REDPANDA_MIGRATION.md) for current setup.
>
> The integration patterns described here (Outbox, Idempotency) remain valid, but infrastructure references (ports, containers) are outdated.

**Status:** ‚úÖ **Production-Ready** (Producer + Consumer + Full Test Coverage)  
**Last Updated:** November 9, 2025 (Archived: November 10, 2025)

## ‚úÖ Files Created/Modified

### Producer Side (Outbox Pattern)

#### 1. **KafkaOutboxMessagePublisher.kt** (NEW)
- Location: `identity-infrastructure/src/main/kotlin/.../outbox/KafkaOutboxMessagePublisher.kt`
- Implements: `OutboxMessagePublisher` interface
- Features:
  - Kafka message publishing with headers
  - Partition routing by `aggregateId` (ensures event ordering per aggregate)
  - Metrics: `@Counted` and `@Timed` on publish attempts
  - Headers: `event-type`, `event-version`, `trace-id`, `tenant-id`, `aggregate-id`
  - ACK/NACK callbacks for observability

#### 2. **OutboxEventScheduler.kt** (UPDATED)
- Added: `MeterRegistry` injection
- Added metrics:
  - `identity.outbox.events.published{outcome=success}`
  - `identity.outbox.events.published{outcome=failure}`
  - `identity.outbox.batch.duration`
- Added: Batch summary logging (success/failure counts)

#### 3. **OutboxEventEntity.kt** (UPDATED)
- Added: `version` column (INT NOT NULL DEFAULT 1)
- Purpose: Event schema evolution tracking per ADR-007

### Consumer Side (Idempotency Pattern)

#### 4. **IdentityEventConsumer.kt** (NEW)
- Location: `identity-infrastructure/src/main/kotlin/.../consumer/IdentityEventConsumer.kt`
- Annotations: `@Blocking`, `@Transactional`
- Features:
  - Header extraction: `event-type`, `event-version`, `aggregate-id`
  - SHA-256 fingerprint: `eventType|version|aggregateId|payload`
  - Duplicate detection via `ProcessedEventRepository`
  - Returns `Uni<Void>` for framework ACK/NACK
  - Error logging with DLQ routing

#### 5. **ProcessedEventEntity.kt** (NEW)
- Table: `identity_processed_events`
- Columns: `id`, `fingerprint` (unique), `processed_at`
- Purpose: Durable idempotency store

#### 6. **ProcessedEventRepository.kt** (NEW)
- Interface for idempotency checks
- Methods: `alreadyProcessed(fingerprint)`, `markProcessed(fingerprint)`

#### 7. **V006__eda_processed_events_and_outbox_version.sql** (NEW)
- Adds `version` column to `identity_outbox_events`
- Creates `identity_processed_events` table with unique fingerprint constraint
- Backfills version=1 for existing outbox events

### Test Infrastructure

#### 8. **IdentityEventConsumerKafkaIT.kt** (NEW)
- **5 comprehensive integration tests** using Testcontainers Kafka
- Tests:
  1. Duplicate detection (same fingerprint ignored)
  2. Different event-type creates distinct fingerprint
  3. Different aggregate-id creates distinct fingerprint
  4. Different event-version creates distinct fingerprint
  5. Different payload creates distinct fingerprint
- Validates: End-to-end header extraction, fingerprinting, JPA persistence

#### 9. **KafkaTestResource.kt** (NEW)
- Testcontainers lifecycle manager
- Starts embedded Kafka (Confluent 7.5.0)
- Injects bootstrap servers into Quarkus config

### Build Configuration

#### 10. **build.gradle.kts** (UPDATED)
- Added dependencies:
  - `io.quarkus:quarkus-messaging-kafka`
  - `org.testcontainers:kafka:1.20.1`

### Application Configuration

#### 11. **application.properties** (UPDATED)
- **Outgoing channel:** `identity-events-out`
  - Topic: `identity.domain.events.v1`
  - Producer settings: `acks=all`, `retries=3`, `enable-idempotence=true`, `compression-type=snappy`
- **Incoming channel:** `identity-events-in`
  - Topic: `identity.domain.events.v1`
  - Consumer group: `identity-consumer`
  - Auto-offset-reset: `earliest`

---

## üîß How It Works

### Complete Event Flow (Producer ‚Üí Kafka ‚Üí Consumer):
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ PRODUCER SIDE ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                      ‚îÇ
‚îÇ  Domain Event ‚Üí EventPublisherPort                  ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ              OutboxEventEntity (DB)                  ‚îÇ
‚îÇ              [PENDING, version=1]                    ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ         OutboxEventScheduler (@Scheduled 5s)         ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ         KafkaOutboxMessagePublisher                  ‚îÇ
‚îÇ         [Headers: event-type, event-version,         ‚îÇ
‚îÇ          aggregate-id, tenant-id, trace-id]          ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ              Kafka Topic: identity.domain.events.v1  ‚îÇ
‚îÇ                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
                       ‚îÇ (Kafka Broker)
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                      ‚îÇ
‚îÇ                CONSUMER SIDE                         ‚îÇ
‚îÇ                                                      ‚îÇ
‚îÇ              IdentityEventConsumer                   ‚îÇ
‚îÇ              [@Incoming, @Blocking, @Transactional]  ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ         Extract Headers (event-type, version, etc.)  ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ         Generate Fingerprint (SHA-256)               ‚îÇ
‚îÇ         "eventType|version|aggregateId|payload"      ‚îÇ
‚îÇ                        ‚Üì                             ‚îÇ
‚îÇ         Check: alreadyProcessed(fingerprint)?        ‚îÇ
‚îÇ              ‚Üì                        ‚Üì              ‚îÇ
‚îÇ            YES                       NO              ‚îÇ
‚îÇ           (Skip)              markProcessed()         ‚îÇ
‚îÇ                                    ‚Üì                 ‚îÇ
‚îÇ                          Process Event Logic         ‚îÇ
‚îÇ                          (Update Read Models)        ‚îÇ
‚îÇ                                    ‚Üì                 ‚îÇ
‚îÇ                      identity_processed_events       ‚îÇ
‚îÇ                      [fingerprint, processed_at]     ‚îÇ
‚îÇ                                                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Idempotency Guarantee

**Fingerprint Calculation:**
```kotlin
SHA-256("$eventType|$version|${aggregateId ?: "n/a"}|$payload")
```

**Dimensions:**
- `event-type`: Distinguishes different domain events
- `event-version`: Enables schema evolution
- `aggregate-id`: Tracks specific entity instance
- `payload`: Content-based deduplication

**Storage:** Unique constraint on `fingerprint` in `identity_processed_events` table prevents duplicates at DB level.

### CDI Wiring (Automatic)
Quarkus automatically wires:
1. `KafkaOutboxMessagePublisher` ‚Üí `OutboxEventScheduler` (producer)
2. `ProcessedEventRepository` ‚Üí `IdentityEventConsumer` (consumer)
3. Both are `@ApplicationScoped` with interface-based injection

**No manual wiring needed!** üéâ

---

## üß™ Testing Locally

### Start Kafka with Docker:

```bash
# Using docker-compose (recommended)
cat > docker-compose-kafka.yml <<EOF
version: '3'
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
EOF

docker-compose -f docker-compose-kafka.yml up -d
```

### Verify Kafka is Running:

```bash
# Check logs
docker logs -f $(docker ps -q --filter ancestor=confluentinc/cp-kafka)

# List topics (should auto-create identity.domain.events.v1)
docker exec -it $(docker ps -q --filter ancestor=confluentinc/cp-kafka) \
  kafka-topics --bootstrap-server localhost:9092 --list
```

### Consume Events:

```bash
# Terminal 1: Consume events with headers
docker exec -it $(docker ps -q --filter ancestor=confluentinc/cp-kafka) \
  kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic identity.domain.events.v1 \
  --property print.headers=true \
  --property print.key=true \
  --from-beginning
```

### Generate Test Events:

```bash
# Terminal 2: Create a user (triggers UserCreatedEvent)
curl -X POST http://localhost:8080/api/identity/users \
  -H "Content-Type: application/json" \
  -H "X-Trace-ID: test-trace-123" \
  -H "X-Tenant-ID: a1b2c3d4-e5f6-7890-abcd-ef1234567890" \
  -d '{
    "tenantId": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
    "username": "john_doe",
    "email": "john@example.com",
    "fullName": "John Doe",
    "password": "SecurePassword123!"
  }'
```

### Expected Output (Consumer Terminal):

```
event-type:UserCreated,trace-id:test-trace-123,tenant-id:a1b2c3d4-...,aggregate-id:9f8e7d6c-...
key:9f8e7d6c-5b4a-3c2d-1e0f-9a8b7c6d5e4f
{
  "eventId": "...",
  "eventType": "UserCreated",
  "occurredAt": "2025-11-06T08:45:00Z",
  "tenantId": "a1b2c3d4-...",
  "userId": "9f8e7d6c-...",
  "username": "john_doe",
  "email": "john@example.com",
  "status": "PENDING"
}
```

---

## üìä Verify Metrics

```bash
# Check Prometheus metrics
curl http://localhost:8080/metrics | grep identity.outbox

# Expected output:
# identity_outbox_events_published_total{outcome="success",} 5.0
# identity_outbox_events_published_total{outcome="failure",} 0.0
# identity_outbox_batch_duration_seconds_count 12.0
# identity_outbox_batch_duration_seconds_sum 0.456
# identity_outbox_publish_attempts_total 5.0
# identity_outbox_publish_duration_seconds_count 5.0
```

---

## üîç Verify Database

```sql
-- Check outbox events are marked as published
SELECT 
  event_id, 
  event_type, 
  status, 
  published_at,
  failure_count
FROM identity_outbox_events
ORDER BY recorded_at DESC
LIMIT 10;

-- Should see status='PUBLISHED' and published_at timestamp
```

---

## üö® Troubleshooting

### Issue: "Failed to connect to Kafka"
**Solution:**
- Check Kafka is running: `docker ps | grep kafka`
- Verify port 9092 is exposed
- Check `kafka.bootstrap.servers` in `application.properties`

### Issue: "Topic not found"
**Solution:**
- Kafka auto-creates topics by default
- Check topic exists: `kafka-topics --list`
- Manually create if needed:
  ```bash
  kafka-topics --create \
    --bootstrap-server localhost:9092 \
    --topic identity.domain.events.v1 \
    --partitions 3 \
    --replication-factor 1
  ```

### Issue: "Events stuck in PENDING"
**Solution:**
- Check scheduler logs: Should see "Dispatching X outbox events"
- Check Kafka connection in app logs
- Verify transaction commits (check database)

### Issue: "Duplicate events published"
**Solution:**
- Idempotence is enabled (`enable-idempotence=true`)
- Check `acks=all` is set
- Verify outbox events are marked published

---

## ‚úÖ Success Criteria

### Producer Verification
- [x] Kafka container running on localhost:9092
- [x] Application starts without errors
- [x] User creation triggers event in outbox table
- [x] Scheduler processes outbox every 5s
- [x] Events published to Kafka topic
- [x] Headers present: `event-type`, `event-version`, `trace-id`, `tenant-id`, `aggregate-id`
- [x] Metrics available at `/metrics`
- [x] Database shows `status='PUBLISHED'` and `published_at` timestamp
- [x] Outbox entity has `version` column

### Consumer Verification
- [x] Consumer processes incoming Kafka messages
- [x] Headers extracted correctly (event-type, version, aggregate-id)
- [x] Fingerprint generated: SHA-256(eventType|version|aggregateId|payload)
- [x] Duplicate events ignored (idempotency)
- [x] Unique events stored in `identity_processed_events`
- [x] `@Blocking` + `@Transactional` annotations present
- [x] Returns `Uni<Void>` for proper ACK/NACK

### Test Coverage
- [x] Unit test: Basic idempotency (`IdentityEventConsumerTest`)
- [x] Integration test: Duplicate detection with real Kafka
- [x] Integration test: Different event-type creates distinct fingerprint
- [x] Integration test: Different aggregate-id creates distinct fingerprint
- [x] Integration test: Different event-version creates distinct fingerprint
- [x] Integration test: Different payload creates distinct fingerprint
- [x] Testcontainers Kafka infrastructure (`KafkaTestResource`)
- [x] All tests passing with ktlint compliance

### Database Schema
- [x] V006 migration: `identity_outbox_events.version` column added
- [x] V006 migration: `identity_processed_events` table created
- [x] Unique constraint on `fingerprint` column
- [x] Index on `processed_at` for housekeeping queries

---

## üß™ Running Tests

### All Identity Infrastructure Tests
```bash
./gradlew :bounded-contexts:tenancy-identity:identity-infrastructure:test
```

### Only Kafka Integration Tests
```bash
./gradlew :bounded-contexts:tenancy-identity:identity-infrastructure:test --tests "IdentityEventConsumerKafkaIT"
```

### With KtLint Check
```bash
./gradlew :bounded-contexts:tenancy-identity:identity-infrastructure:test \
          :bounded-contexts:tenancy-identity:identity-infrastructure:ktlintCheck
```

### Architecture Tests (Global Gates)
```bash
./gradlew :tests:arch:test
```

---

## üìù Commit Message Template

```bash
git add bounded-contexts/tenancy-identity/identity-infrastructure/
git add docs/KAFKA_INTEGRATION_SUMMARY.md

git commit -m "feat(identity-eda): complete producer + consumer with idempotency

Producer (Outbox Pattern):
- KafkaOutboxMessagePublisher with header injection
- OutboxEventEntity.version column for schema evolution
- Micrometer metrics for observability
- Headers: event-type, event-version, aggregate-id, tenant-id, trace-id
- Partition by aggregateId for ordering guarantees

Consumer (Idempotency Pattern):
- IdentityEventConsumer with @Blocking + @Transactional
- SHA-256 fingerprint: eventType|version|aggregateId|payload
- ProcessedEventEntity table with unique fingerprint constraint
- Duplicate detection via alreadyProcessed() check
- Returns Uni<Void> for framework ACK/NACK

Database:
- V006 migration: outbox.version + identity_processed_events table
- Unique constraint on fingerprint for DB-level deduplication
- Index on processed_at for housekeeping queries

Testing:
- IdentityEventConsumerKafkaIT: 5 comprehensive fingerprint tests
- KafkaTestResource: Testcontainers embedded Kafka
- All dimensions validated: event-type, version, aggregateId, payload
- Full end-to-end coverage with real Kafka headers

Compliance:
- ADR-007 EDA Hybrid Policy (Producer + Consumer patterns)
- KtLint formatting validated
- ArchUnit global gates passing

Kafka config:
- Producer: acks=all, retries=3, idempotence=true, compression=snappy
- Consumer: group=identity-consumer, auto-offset-reset=earliest
- Topic: identity.domain.events.v1"
```

---

## üéØ Next Steps

### Short Term
1. **Monitor in Dev**: Watch consumer logs for duplicate detection in action
2. **Add Business Logic**: Replace placeholder in consumer with real event handlers
3. **Housekeeping Job**: Add cleanup for `identity_processed_events` older than 30 days

### Medium Term
1. **Port to Other Contexts**: Use this as template for Commerce, Inventory, Financial per ADR-007
2. **Add DLQ Monitoring**: Alert on messages landing in `identity.domain.events.dlq`
3. **Consumer Metrics**: Add processing duration, duplicate rate metrics

### Long Term
1. **Event Replay**: Build admin tool to replay events from processed_events table
2. **Schema Registry**: Integrate Confluent Schema Registry for payload validation
3. **Event Sourcing**: Extend pattern to event-sourced aggregates where applicable

---

**Status:** ‚úÖ **Production-Ready**  
**EDA Maturity Level:** **Level 3** (Choreography + Idempotency + Full Testing)  
**Next Context:** Commerce (Order-to-Cash flow per ADR-007 rollout plan)
